{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnabanupam/UNET/blob/master/UnetModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YIZTliYNLgk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e32375a8-9747-4076-c05a-a25ea49967ab"
      },
      "source": [
        "import numpy as np \n",
        "import os\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "import numpy as np\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout,  concatenate, UpSampling2D\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as keras\n",
        "from keras import losses as l\n",
        "from keras import optimizers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmNAiVJxQPBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smooth=1\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = keras.flatten(y_true)\n",
        "    y_pred_f = keras.flatten(y_pred)\n",
        "    intersection = keras.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (keras.sum(y_true_f*y_true_f) + keras.sum(y_pred_f*y_pred_f) + smooth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x12qaMJ7QWUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unet(pretrained_weights = None,input_size = (256,256,1)):\n",
        "    inputs = Input(input_size)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "    merge6 = concatenate([drop4,up6], axis = 3)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "\n",
        "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQlGDzViQeq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " model = Model(input = inputs, output = conv10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4wUZ2hHQhut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " model.compile(optimizer = Adam(lr = 1e-4), loss = l.binary_crossentropy, metrics = ['dice_coef'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJDUBMYkQnE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if(pretrained_weights):\n",
        "    \tmodel.load_weights(pretrained_weights)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD9qzsMGQ7Ox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO_3fUVOQ7he",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBulhaIKQ8mU",
        "colab_type": "text"
      },
      "source": [
        "# PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpWzOxvWRBOR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "fbd0e4e6-ad1e-41f5-e3bd-0479cc7e09ca"
      },
      "source": [
        "import cv2\n",
        "import h5py\n",
        "import glob\n",
        "import SimpleITK as sitk\n",
        "\n",
        "from keras import callbacks as K\n",
        "from keras import backend as kb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.engine import Model\n",
        "import tensorlayer as tl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4433944973f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorlayer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorlayer/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             LooseVersion(tensorflow.__version__) < LooseVersion(\"2.0.0\")):\n\u001b[1;32m     26\u001b[0m         raise RuntimeError(\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;34m\"TensorLayer does not support Tensorflow version older than 2.0.0.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;34m\"Please update Tensorflow with:\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;34m\" - `pip install --upgrade tensorflow`\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: TensorLayer does not support Tensorflow version older than 2.0.0.\nPlease update Tensorflow with:\n - `pip install --upgrade tensorflow`\n - `pip install --upgrade tensorflow-gpu`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06uhNp5pTOMn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "outputId": "ba94787d-f169-4228-fb09-d7a5215f27e5"
      },
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 205kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 19.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.1)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.7)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.0rc1)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.15.4)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (41.0.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\n",
            "Installing collected packages: tensorboard, tensorflow\n",
            "  Found existing installation: tensorboard 1.13.1\n",
            "    Uninstalling tensorboard-1.13.1:\n",
            "      Successfully uninstalled tensorboard-1.13.1\n",
            "  Found existing installation: tensorflow 1.14.0rc1\n",
            "    Uninstalling tensorflow-1.14.0rc1:\n",
            "      Successfully uninstalled tensorflow-1.14.0rc1\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4r8hZE_RNtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dataset = 'HGG'p\n",
        "p = os.listdir(Dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrALUbMKRz_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "F1=[glob.glob(Dataset+ '/' + p1+('/*flair.nii*'))[0] for p1 in p[:]]\n",
        "T1=[glob.glob(Dataset+ '/' + p1+('/*t1.nii*'))[0] for p1 in p[:]]\n",
        "TCE=[glob.glob(Dataset+ '/' + p1+('/*t1ce.nii*'))[0] for p1 in p[:]]\n",
        "T2=[glob.glob(Dataset+ '/' + p1+('/*t2.nii*'))[0] for p1 in p[:]]\n",
        "SG=[glob.glob(Dataset+ '/' + p1+('/*seg.nii*'))[0] for p1 in p[:]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gV6Izg2R37b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_F1=[]\n",
        "data_T1=[]\n",
        "data_TCE=[]\n",
        "data_T2=[]\n",
        "data_SG=[]\n",
        "data_X=[]\n",
        "data_Y=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyFOqb4FR7Un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(F1)):\n",
        "  \n",
        "    X = sitk.ReadImage(F1[i])  \n",
        "    img = sitk.GetArrayFromImage(X)\n",
        "    img=np.asarray(img,'float32')\n",
        "#    print(np.shape(img))\n",
        "    img=cv2.resize(img,(240,240))\n",
        "    img=np.transpose(img,(1,2,0))\n",
        "    img=cv2.resize(img,(240,240))\n",
        "    img=np.transpose(img,(2,0,1))\n",
        "    img=np.asarray(img,'int')\n",
        "    img=np.asarray(img,'float32')\n",
        "\n",
        "    img1=img\n",
        "    #    print(np.shape(img))\n",
        "    for j in range(img.shape[0]):        \n",
        "        if (np.sum(img1[j])>0):\n",
        "            data_F1.append(img[j])\n",
        "\n",
        "\n",
        "    X = sitk.ReadImage(T1[i])\n",
        "    img = sitk.GetArrayFromImage(X)\n",
        "    img=np.asarray(img,'float32')\n",
        "    img=cv2.resize(img,(240,240))\n",
        "    img=np.transpose(img,(1,2,0))\n",
        "    img=cv2.resize(img,(240,240))\n",
        "    img=np.transpose(img,(2,0,1))\n",
        "    img=np.asarray(img,'int')\n",
        "    img=np.asarray(img,'float32')\n",
        "    img1=img\n",
        "#    print(np.shape(img))\n",
        "\n",
        "    for j in range(img.shape[0]):        \n",
        "        if (np.sum(img1[j])>0):\n",
        "            data_T1.append(img[j])\n",
        " \n",
        "\n",
        "    X = sitk.ReadImage(T2[i])\n",
        "    img = sitk.GetArrayFromImage(X)\n",
        "    img=np.asarray(img,'float32')\n",
        "    img=cv2.resize(img,(240,240))\n",
        "    img=np.transpose(img,(1,2,0))\n",
        "    img=cv2.resize(img,(240,240))\n",
        "    img=np.transpose(img,(2,0,1))\n",
        "    img1=img\n",
        " \n",
        "#    print(np.shape(img))\n",
        "\n",
        "    for j in range(img.shape[0]):        \n",
        "        if (np.sum(img1[j])>0):\n",
        "            data_T2.append(img[j])\n",
        " \n",
        "\n",
        "    X = sitk.ReadImage(SG[i])\n",
        "    img = sitk.GetArrayFromImage(X)\n",
        "    img=np.asarray(img,'float32')\n",
        "    img=cv2.resize(img,(240,240))\n",
        "    img=np.transpose(img,(1,2,0))\n",
        "    img=cv2.resize(img,(240,240))\n",
        "    img=np.transpose(img,(2,0,1))\n",
        "    img=np.asarray(img,'int')\n",
        "    img=np.asarray(img,'float32')\n",
        "#    print(np.shape(img))\n",
        "\n",
        "    for j in range(img.shape[0]):        \n",
        "        if (np.sum(img1[j])>0):\n",
        "            data_SG.append(img[j])\n",
        "    \n",
        "    X = sitk.ReadImage(TCE[i])\n",
        "    img = sitk.GetArrayFromImage(X)\n",
        "    img=np.asarray(img,'float32')\n",
        "    img=cv2.resize(img,(240,240))\n",
        "    img=np.transpose(img,(1,2,0))\n",
        "    img=cv2.resize(img,(240,240))\n",
        "    img=np.transpose(img,(2,0,1))\n",
        "    img=np.asarray(img,'int')\n",
        "    img=np.asarray(img,'float32')\n",
        "#    print(np.shape(img))\n",
        "\n",
        "    for j in range(img.shape[0]):        \n",
        "        if (np.sum(img1[j])>0):\n",
        "            data_TCE.append(img[j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crHx-eCISAFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_F1 = np.array(data_F1)\n",
        "data_T1 = np.array(data_T1)\n",
        "data_SG = np.array(data_SG)\n",
        "data_TCE = np.array(data_TCE)\n",
        "data_T2 = np.array(data_T2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy37N5VzSCeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name=\"mydata\"\n",
        "hf=h5py.File(name+'.h5','w')\n",
        "hf.create_dataset('f1_data', data=data_F1)\n",
        "hf.create_dataset('t1_data', data=data_T1)\n",
        "hf.create_dataset('SG_data', data=data_SG)\n",
        "hf.create_dataset('t2_data', data=data_T2)\n",
        "hf.create_dataset('TCE_data', data=data_TCE)\n",
        "hf.create_dataset('X_data', data=data_X)\n",
        "hf.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmBY20I0SFjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def distort_imgs(data_F1,data_TCE,data_SG,data_T1,data_TCE):\n",
        "    \"\"\" data augumentation \"\"\"\n",
        "    x1, x2, x3, x4,x5, y = data_F1,data_TCE,data_SG,data_T1,data_T2\n",
        "    # x1, x2, x3, x4, y = tl.prepro.flip_axis_multi([x1, x2, x3, x4, y],  # previous without this, hard-dice=83.7\n",
        "    #                         axis=0, is_random=True) # up down\n",
        "    x1, x2, x3, x4,x5, y = tl.prepro.flip_axis_multi([x1, x2, x3, x4,x5, y],\n",
        "                            axis=1, is_random=True) # left right\n",
        "#    x1, x2, x3, x4,x5, y = tl.prepro.elastic_transform_multi([x1, x2, x3, x4, x5, y],\n",
        "#                            alpha=720, sigma=24, is_random=True)\n",
        "    x1, x2, x3, x4,x5, y = tl.prepro.rotation_multi([x1, x2, x3, x4,x5, y], rg=20,\n",
        "                            is_random=True, fill_mode='constant', row_index=1, col_index=2, channel_index=0) # nearest, constant\n",
        "#    x1, x2, x3, x4,x5, y = tl.prepro.shift_multi([x1, x2, x3, x4,x5, y], wrg=0.10,\n",
        "#                            hrg=0.10, is_random=True, fill_mode='constant')\n",
        "    x1, x2, x3, x4,x5, y = tl.prepro.shear_multi([x1, x2, x3, x4,x5, y], 0.05,\n",
        "                            is_random=True, fill_mode='constant', row_index=1, col_index=2, channel_index=0)\n",
        "#    x1, x2, x3, x4,x5, y = tl.prepro.zoom_multi([x1, x2, x3, x4,x5, y],\n",
        "#                            zoom_range=[0.9, 1.1], is_random=True,\n",
        "#                            fill_mode='constant')\n",
        "    return x1, x2, x3, x4, x5, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugELSzp4SKGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_F11,data_TCE1,data_SG1,data_T11,data_T21 = distort_imgs(data_F1,data_TCE,data_SG,data_T1,data_T2)\n",
        "data_F12,data_TCE2,data_SG2,data_T12,data_T22 = distort_imgs(data_F1,data_TCE,data_SG,data_T1,data_T2)\n",
        "\n",
        "data_SG=np.concatenate((data_SG,data_SG1,data_SG2),axis=0)\n",
        "data_F1=np.concatenate((data_F1,data_F11,data_F12),axis=0)\n",
        "data_TCE=np.concatenate((data_TCE,data_TCE1,data_TCE2),axis=0)\n",
        "data_T1=np.concatenate((data_T1,data_T11,data_T12),axis=0)\n",
        "data_T2=np.concatenate((data_T2,data_T21,data_T22),axis=0)\n",
        "\n",
        "del data_SG1, data_SG2, data_F11, data_F12, data_TCE1, data_TCE2, data_T11, data_T12, data_T21, data_T22\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfoPRX2rSP3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(data_F1)):\n",
        "    data_SG[i]=(data_SG[i]-np.mean(data_SG[i])+0.00001)/(np.std(data_SG[i])+0.00001)\n",
        "    data_F1[i]=(data_F1[i]-np.mean(data_F1[i])+0.00001)/(np.std(data_F1[i])+0.00001)\n",
        "    data_TCE[i]=(data_TCE[i]-np.mean(data_TCE[i])+0.00001)/(np.std(data_TCE[i])+0.00001)\n",
        "    data_T1[i]=(data_T1[i]-np.mean(data_T1[i])+0.00001)/(np.std(data_T1[i])+0.00001)\n",
        "    data_T2[i]=(data_T2[i]-np.mean(data_T2[i])+0.00001)/(np.std(data_T2[i])+0.00001)\n",
        "data_X=[]\n",
        "data_Y=[]\n",
        "\n",
        "for i in range(len(data_F1)):\n",
        "    k1=data_SG[i]\n",
        "    k1=k1[:,:,np.newaxis,]\n",
        "    k2=data_F1[i]\n",
        "    k2=k2[:,:,np.newaxis,]\n",
        "    k3=data_TCE[i]\n",
        "    k3=k3[:,:,np.newaxis,]\n",
        "    k4=data_T1[i]\n",
        "    k4=k4[:,:,np.newaxis,] \n",
        "    k5=data_T2[i]\n",
        "    k5=k5[:,:,np.newaxis,] \n",
        "    k6=np.concatenate((k1,k2,k3,k4,k5),axis=-1)=\n",
        "    if(np.sum(k6)!=0):\n",
        "        data_X.append(k6)\n",
        "        data_Y.append(data_OT[i])\n",
        "\n",
        "data_X = np.array(data_X)\n",
        "data_Y = np.array(data_Y)\n",
        "name=\"mydata_aug\"\n",
        "hf=h5py.File(name+'.h5','w')\n",
        "hf.create_dataset('f1_data', data=data_F1)\n",
        "hf.create_dataset('t1_data', data=data_T1)\n",
        "hf.create_dataset('DP_data', data=data_SG)\n",
        "hf.create_dataset('t2_data', data=data_T2)\n",
        "hf.create_dataset('G1_data', data=data_TCE)\n",
        "hf.create_dataset('X_data', data=data_X)\n",
        "hf.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9XhPSbvST0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_Y=data_Y[:,:,:,np.newaxis,]\n",
        "for i in range(len(data_Y)):\n",
        "    res=np.where(data_Y[i,:,:,0]>0)\n",
        "    for j in range(len(res[0])):\n",
        "        data_Y[i,res[0][j],res[1][j],0]=1.0\n",
        "#    res=np.where(data_Y[i,:,:,0]<0.5)\n",
        "#    for j in range(len(res[0])):\n",
        "#        data_Y[i,res[0][j],res[1][j],0]=0.0\n",
        "\n",
        "del data_SG, data_TCE, data_F1,data_T1, data_T2            \n",
        "#for i in range(6414):\n",
        "#    print(np.max(data_Y[i,:,:]))\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y, test_size=0.20, random_state=42)\n",
        "name=\"mydata_training_testing_result\"\n",
        "hf=h5py.File(name+'.h5','w')\n",
        "hf.create_dataset('X_train', data=X_train)\n",
        "hf.create_dataset('Y_train', data=Y_train)\n",
        "hf.create_dataset('X_test', data=X_test)\n",
        "hf.create_dataset('Y_test', data=Y_test)\n",
        "#hf.create_dataset('pred_result', data=pred_result)\n",
        "#hf.create_dataset('X_data', data=data_X)\n",
        "hf.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLoYgFAvSdsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unetmodel = unet()\n",
        "with h5py.File('mydata_training_testing_result.h5','r') as hf:\n",
        "    print(list(hf.keys()))\n",
        "    X_test=hf['X_test'][:]\n",
        "    Y_test=hf['Y_test'][:]\n",
        "    Y_train=hf['Y_train'][:]\n",
        "    X_train=hf['X_train'][:]\n",
        "    hf.close()\n",
        "#model_checkpoint = K.ModelCheckpoint('unet.hdf5', monitor='loss',verbose=1, save_best_only=False)\n",
        "model_checkpoint = K.ModelCheckpoint(\n",
        "        filepath=\"weights.{epoch:04d}.h5\",\n",
        "        monitor=\"loss\",\n",
        "        save_best_only=False)\n",
        "\n",
        "unetmodel.load_weights('weights.0009.h5')\n",
        "unetmodel.fit(X_train, Y_train, batch_size=3, epochs=50,validation_data=(X_test,Y_test), shuffle=True,callbacks=[model_checkpoint],verbose=1,initial_epoch=11)\n",
        "unetmodel.load_weights('weights.0037.h5')\n",
        "unetmodel.evaluate(x=X_test, y=Y_test,batch_size=16, verbose=1)\n",
        "pred_result = unetmodel.predict(X_train,batch_size=16)\n",
        "\n",
        "name=\"mydata_train_data_result\"\n",
        "hf=h5py.File(name+'.h5','w')\n",
        "hf.create_dataset('X_train', data=X_train)\n",
        "hf.create_dataset('Y_train', data=Y_train)\n",
        "hf.create_dataset('pred_result', data=pred_result)\n",
        "hf.close()\n",
        "\n",
        "with h5py.File('mydata_testing_data_result.h5','r') as hf:\n",
        "    print(list(hf.keys()))\n",
        "    X_test=hf['X_test'][:]\n",
        "    Y_test=hf['Y_test'][:]\n",
        "    pred_result=hf['pred_result'][:]\n",
        "    hf.close()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}